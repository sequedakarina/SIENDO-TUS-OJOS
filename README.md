# SIENDO-TUS-OJOS

<b>INTEGRANTES:</b><br>     VERÓNICA LUCENA (2152903)  
                            KARINA SEQUEDA (2152476) 
                            
<b>OBJETIVO:</b> Brindar una herramienta de apoyo a aquellas personas con discapacidad visual mediante el reconocimiento de caracteres traducidos a señales auditivas.                            
                            
<b>MOTIVACIÓN:</b> Algunos de los estudiantes que integran la comunidad UIS cuentan con discapacidad visual, muchos de ellos no poseen herramientas útiles para consultar la información que requieren, pues no todos los libros se encuetran en sistema braille. Es por esto que se decidió plantear una propuesta de solución, creando un mecanismo que les permite escuchar el texto que necesitan, a través de una fotografía.

<b>MODELO:</b> El proyecto busca plasmar los conocimientos adquiridos en la asignatura respecto a
Deep-Learning implementando redes convolucionales como Lenet5, que es muy útil para clasificar manuscritos, esta red se usa para entrenar y probar datasets como MNIST, además de una CNN adicional con la cual se obtuvo buena precisión.Se aplican conoceptos de Deep Feature y TransferLearning en el dataset (propio) para obtener otras alternativas de clasificación. Se incorporan también transformaciones morfológicas tales como cierre, apertura, dilitación y erosión, que permiten localizar los objetos de interés en una imagen para luego proceder a segmentarlos, logrando así un buen reconocimiento de caracteres, a partir del ingreso de una imagen(Fotografía) con texto, haciendo lectura de este a través del reproductor de voz gTTs (librería de Google).

<b>DATASET:</b> El dataset utilizado es un dataset propio, creado con la colaboración de algunos estudiantes de Ingeniería de Sistemas, a quienes se les pidió que escribieran en unas hojas el Abecedario (excluyendo la ñ) tanto en mayúscula como minúscula. Cuenta con 832 imágenes etiquetada, los archivos "Datos" y "Labels" corresponden a las imágenes y etiquetas del dataset.

 <b>METODOLOGÍA Y DESCRIPCIÓN DE DATOS CAPTURADOS:</b> El proyecto se desarrolla en cinco fases. En la primera fase se emplearán las imágenes del dataset para entrenar y evaluar el modelo con dos redes CNN y una DNN aplicando el concepto de Deep Feature, la primera red CNN contiene dos capas convolucionales, una con 32 y otra con 64 filtros, dos MaxPooling, dos Dropout y una full connected con 128 neuronas, la segunda red CNN (lenet5) continene dos capas convolucionales, una con 6 y otra con 16 filtros, dos MaxPooling, un Dropout y dos full connected, una con 120 y otra con 84 neuronas,la red DNN contiene tres capas una con 300, 200 y 100 neuronas, utilizando como función de activación Sigmoide. Debido a que los resultados obtenidos con la red lenet5 entrenada con MNIST y evaluada con el dataset propio no son óptimos, se procede a aplicar el concepto de Transfer Learning, tomando dicha red para reentrenarla con el dataset propio utilizando pocos epochs. En la segunda fase se realiza el procesamiento de las imágenes que contienen las palabras a analizar, para ello se implementa la transformación morfológica denominada erosión, que permite adelgazar el objeto de interés, seguido de una apertura (elimina los puntos blancos presentes en un fondo negro), dilatación, cierre (elimina puntos negros presentes en un fondo blanco) y nuevamente erosión; después de esto se hace la segmentación. En la tercera fase se pasan los datos a la red neuronal seleccionada. En la cuarta fase se transforman y concatenan los números arrojados por la red neuronal a las letras correspondientes para formar la palabra inicial. Por último, en la quinta fase, se toma el vector con la palabra final y se pasa a la librería que la leerá. 

